{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eeb2621",
   "metadata": {},
   "source": [
    "# Step 1: Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3112898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import everything necessary\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56e2082",
   "metadata": {},
   "source": [
    "# Step 2: Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09e6482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dataset & confirm everything is alright\n",
    "df = pd.read_csv(\"work_with_this_dataset.csv\")\n",
    "print(df.head(100))\n",
    "print(df.describe())\n",
    "print(df[\"Trend\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f56586",
   "metadata": {},
   "source": [
    "# Step 3: Pre-processing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec3d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing steps\n",
    "#Standardize\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=df.columns, index=df.index)\n",
    "#print(df_scaled.head()) #This is to make sure everything happened correctly\n",
    "\n",
    "#Split\n",
    "X = df_scaled.drop('Trend', axis=1)\n",
    "y = df_scaled['Trend']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eb5494",
   "metadata": {},
   "source": [
    "# Step 4: Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86192852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline logistic regression model\n",
    "model_LR = LogisticRegression(random_state=42)\n",
    "model_LR.fit(X_train, y_train)\n",
    "LR_pred = model_LR.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, LR_pred))\n",
    "print(precision_score(y_test, LR_pred, average='weighted'))\n",
    "print(recall_score(y_test, LR_pred, average='weighted'))\n",
    "print(f1_score(y_test, LR_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f1dd26",
   "metadata": {},
   "source": [
    "# Step 5.1: Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b4672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model Hyperparameter Tuning\n",
    "\n",
    "n_estimators_range = range(150, 301, 10)\n",
    "max_depth_range = range(20, 31, 5)\n",
    "min_samples_split_range = [2, 5, 10]\n",
    "min_samples_leaf_range = [1, 2, 4]\n",
    "\n",
    "param_grid = product(n_estimators_range, max_depth_range, min_samples_split_range, min_samples_leaf_range)\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "best_params = None\n",
    "\n",
    "for n_estimators, max_depth, min_samples_split, min_samples_leaf in param_grid:\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, random_state=42, n_jobs=-1)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\nParams: n_estimators={n_estimators}, max_depth={max_depth}, \"f\"min_samples_split={min_samples_split}, min_samples_leaf={min_samples_leaf}\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        best_model = model\n",
    "        best_params = (n_estimators, max_depth, min_samples_split, min_samples_leaf)\n",
    "\n",
    "\n",
    "print(f\"\\nBest Model Parameters: {best_params}\")\n",
    "print(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, best_model.predict(X_test)))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, best_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e316a07e",
   "metadata": {},
   "source": [
    "This is what above code outputted (so you don't need to run it again):\n",
    "\n",
    "Best Model Parameters: (300, 30, 10, 4)\n",
    "Best Accuracy: 0.6907\n",
    "Confusion Matrix:\n",
    " [[12166  8918]\n",
    " [ 4940 18782]]\n",
    "Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.71      0.58      0.64     21084\n",
    "         1.0       0.68      0.79      0.73     23722\n",
    "\n",
    "    accuracy                           0.69     44806\n",
    "   macro avg       0.69      0.68      0.68     44806\n",
    "weighted avg       0.69      0.69      0.69     44806"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5b5432",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest model\n",
    "model_RFC = RandomForestClassifier(n_estimators=300, max_depth=30, min_samples_split=10, min_samples_leaf=4, random_state=42)\n",
    "model_RFC.fit(X_train, y_train)\n",
    "RFC_pred = model_RFC.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, RFC_pred))\n",
    "print(precision_score(y_test, RFC_pred, average='weighted', zero_division=1))\n",
    "print(recall_score(y_test, RFC_pred, average='weighted'))\n",
    "print(f1_score(y_test, RFC_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aed8e6",
   "metadata": {},
   "source": [
    "Above cell outputted:\n",
    "\n",
    "0.6907110654823014\n",
    "0.6936552701085712\n",
    "0.6907110654823014\n",
    "0.6865660511611431"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d43e47",
   "metadata": {},
   "source": [
    "# Step 5.2: XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6992fed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Tuning XGBoost\n",
    "\n",
    "n_estimators = range(50, 101, 10)\n",
    "max_depth = range(3, 5)\n",
    "max_leaves = range(0, 16)\n",
    "grow_policy = ['depthwise', 'lossguide']\n",
    "\n",
    "param_grid_XG = product(n_estimators, max_depth, max_leaves, grow_policy)\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "best_params = None\n",
    "\n",
    "for n_estimators, max_depth, max_leaves, grow_policy in param_grid_XG:\n",
    "    if grow_policy == 'depthwise' and max_leaves > 0:\n",
    "        continue\n",
    "    if grow_policy == 'lossguide' and max_leaves == 0:\n",
    "        continue\n",
    "\n",
    "    model_XGB2 = XGBClassifier(objective='binary:logistic', eval_metric='logloss', random_state=42, n_estimators=n_estimators, max_depth=max_depth, max_leaves=max_leaves, grow_policy=grow_policy, tree_method='hist' if grow_policy == 'lossguide' else 'auto')\n",
    "\n",
    "    model_XGB2.fit(X_train, y_train)\n",
    "    y_pred = model_XGB2.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\nParams: n_estimators={n_estimators}, max_depth={max_depth}, \"f\"max_leaves={max_leaves}, grow_policy={grow_policy}\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        best_model = model_XGB2\n",
    "        best_params = (n_estimators, max_depth, max_leaves, grow_policy)\n",
    "\n",
    "print(f\"\\nBest Model Params: {best_params}\")\n",
    "print(f\"Best Accuracy: {best_accuracy}\")\n",
    "plot_importance(best_model)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c2b687",
   "metadata": {},
   "source": [
    "Eventual output is:\n",
    "\n",
    "Best Model Params: (100, 4, 15, 'lossguide')\n",
    "Best Accuracy: 0.7048163192429585"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c718bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypertuned XGBoost model\n",
    "\n",
    "model_XGB = XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, random_state=42, n_estimators=100, max_depth=4, max_leaves=15, grow_policy='lossguide', tree_method='auto')\n",
    "model_XGB.fit(X_train, y_train)\n",
    "y_pred_XGB = model_XGB.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_XGB))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_XGB))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_XGB))\n",
    "xgb.plot_importance(model_XGB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "block_d_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
